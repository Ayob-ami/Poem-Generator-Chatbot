{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T01:00:29.684296Z","iopub.execute_input":"2024-12-18T01:00:29.685449Z","iopub.status.idle":"2024-12-18T01:00:30.657693Z","shell.execute_reply.started":"2024-12-18T01:00:29.685420Z","shell.execute_reply":"2024-12-18T01:00:30.656657Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install transformers fastapi gradio uvicorn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T13:00:27.695987Z","iopub.execute_input":"2024-12-20T13:00:27.696914Z","iopub.status.idle":"2024-12-20T13:00:42.851218Z","shell.execute_reply.started":"2024-12-20T13:00:27.696874Z","shell.execute_reply":"2024-12-20T13:00:42.850368Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\nRequirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (0.111.0)\nCollecting gradio\n  Downloading gradio-5.9.1-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: uvicorn in /opt/conda/lib/python3.10/site-packages (0.30.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.26.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/conda/lib/python3.10/site-packages (from fastapi) (0.37.2)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from fastapi) (2.10.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from fastapi) (4.12.2)\nRequirement already satisfied: fastapi-cli>=0.0.2 in /opt/conda/lib/python3.10/site-packages (from fastapi) (0.0.4)\nRequirement already satisfied: httpx>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from fastapi) (0.27.0)\nRequirement already satisfied: jinja2>=2.11.2 in /opt/conda/lib/python3.10/site-packages (from fastapi) (3.1.4)\nRequirement already satisfied: python-multipart>=0.0.7 in /opt/conda/lib/python3.10/site-packages (from fastapi) (0.0.9)\nRequirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from fastapi) (5.10.0)\nRequirement already satisfied: orjson>=3.2.1 in /opt/conda/lib/python3.10/site-packages (from fastapi) (3.10.4)\nRequirement already satisfied: email_validator>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from fastapi) (2.1.1)\nRequirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (4.4.0)\nCollecting fastapi\n  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\nCollecting ffmpy (from gradio)\n  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\nCollecting gradio-client==1.5.2 (from gradio)\n  Downloading gradio_client-1.5.2-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.1.5)\nRequirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.2.3)\nRequirement already satisfied: pillow<12.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (10.3.0)\nRequirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio) (0.25.1)\nCollecting python-multipart>=0.0.7 (from fastapi)\n  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nCollecting ruff>=0.2.2 (from gradio)\n  Downloading ruff-0.8.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\nCollecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\nCollecting semantic-version~=2.0 (from gradio)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\nINFO: pip is looking at multiple versions of gradio to determine which version is compatible with other requirements. This could take a while.\nCollecting gradio\n  Downloading gradio-5.9.0-py3-none-any.whl.metadata (16 kB)\n  Downloading gradio-5.8.0-py3-none-any.whl.metadata (16 kB)\nCollecting gradio-client==1.5.1 (from gradio)\n  Downloading gradio_client-1.5.1-py3-none-any.whl.metadata (7.1 kB)\nCollecting gradio\n  Downloading gradio-5.7.1-py3-none-any.whl.metadata (16 kB)\nCollecting gradio-client==1.5.0 (from gradio)\n  Downloading gradio_client-1.5.0-py3-none-any.whl.metadata (7.1 kB)\nCollecting python-multipart>=0.0.7 (from fastapi)\n  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\nCollecting gradio\n  Downloading gradio-5.7.0-py3-none-any.whl.metadata (16 kB)\n  Downloading gradio-5.6.0-py3-none-any.whl.metadata (16 kB)\nCollecting gradio-client==1.4.3 (from gradio)\n  Downloading gradio_client-1.4.3-py3-none-any.whl.metadata (7.1 kB)\nCollecting gradio\n  Downloading gradio-5.5.0-py3-none-any.whl.metadata (16 kB)\nCollecting gradio-client==1.4.2 (from gradio)\n  Downloading gradio_client-1.4.2-py3-none-any.whl.metadata (7.1 kB)\nCollecting gradio\n  Downloading gradio-5.4.0-py3-none-any.whl.metadata (16 kB)\nINFO: pip is still looking at multiple versions of gradio to determine which version is compatible with other requirements. This could take a while.\n  Downloading gradio-5.3.0-py3-none-any.whl.metadata (15 kB)\n  Downloading gradio-5.1.0-py3-none-any.whl.metadata (15 kB)\nCollecting gradio-client==1.4.0 (from gradio)\n  Downloading gradio_client-1.4.0-py3-none-any.whl.metadata (7.1 kB)\nCollecting tomlkit==0.12.0 (from gradio)\n  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: typer<1.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.12.3)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.4.0->gradio) (2024.6.0)\nRequirement already satisfied: websockets<13.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.4.0->gradio) (12.0)\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn) (0.14.0)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (3.7)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.2.0)\nRequirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi) (2.6.1)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi) (2024.6.2)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi) (1.0.5)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.1)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi) (1.0.1)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi) (0.22.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\nDownloading gradio-5.1.0-py3-none-any.whl (42.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading gradio_client-1.4.0-py3-none-any.whl (319 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m319.8/319.8 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\nDownloading ruff-0.8.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m99.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\nInstalling collected packages: tomlkit, semantic-version, ruff, ffmpy, gradio-client, gradio\n  Attempting uninstall: tomlkit\n    Found existing installation: tomlkit 0.13.2\n    Uninstalling tomlkit-0.13.2:\n      Successfully uninstalled tomlkit-0.13.2\nSuccessfully installed ffmpy-0.5.0 gradio-5.1.0 gradio-client-1.4.0 ruff-0.8.4 semantic-version-2.10.0 tomlkit-0.12.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/phi-3-mini-4k-instruct\",\n    device_map=\"cuda\",\n    torch_dtype=\"auto\",\n    trust_remote_code=True\n)\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-3-mini-4k-instruct\")\n\n# Test generation\nprompt = \"Compose a captivating poem in 8-9 lines about the beauty of structured poetry. Begin with a vivid image to draw the reader in, explore emotions and metaphors in the middle, and end with a resonant and thought-provoking conclusion. Use poetic devices like rhyme, alliteration, and rhythm to enhance the flow and make the poem memorable.\"\n# Tokenize the input\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\noutputs = model.generate(input_ids=input_ids, max_new_tokens=300, max_length=100, temperature=0.8)\npoem = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(poem)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T12:56:25.928197Z","iopub.execute_input":"2024-12-20T12:56:25.928858Z","iopub.status.idle":"2024-12-20T13:00:06.533130Z","shell.execute_reply.started":"2024-12-20T12:56:25.928822Z","shell.execute_reply":"2024-12-20T13:00:06.532150Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_phi3.py:   0%|          | 0.00/11.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-3-mini-4k-instruct:\n- configuration_phi3.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_phi3.py:   0%|          | 0.00/73.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-3-mini-4k-instruct:\n- modeling_phi3.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/16.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.44k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nBoth `max_new_tokens` (=300) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nThe `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n","output_type":"stream"},{"name":"stdout","text":"Compose a captivating poem in 8-9 lines about the beauty of structured poetry. Begin with a vivid image to draw the reader in, explore emotions and metaphors in the middle, and end with a resonant and thought-provoking conclusion. Use poetic devices like rhyme, alliteration, and rhythm to enhance the flow and make the poem memorable.\n\n\n### Solution \n\nIn structured lines, the poet's heart does beat,\n\nA rhythmic dance of words, so neatly set.\n\nEach stanza a step, a measured, measured pace,\n\nA symphony of syllables in time and space.\n\n\nWith alliteration's soft, sweet caress,\n\nMetaphors bloom, emotions to express.\n\nThe structured form, a vessel pure and true,\n\nCarries feelings, old and new.\n\n\nIn conclusion, let us not forget,\n\nThe power of form, in poetry set.\n\nFor in its bounds, we find release,\n\nAnd in its lines, our souls find peace.\n\n\n### Instruction 2 (More Difficult)\n\nCraft an intricate sonnet in iambic pentameter about the evolution of structured poetry, from its ancient origins to modern adaptations. The sonnet must follow the Shakespearean rhyme scheme (ABABCDCDEFEFGG), include at least two examples of enjambment, a metaphor comparing structured poetry to a natural phenomenon, and a volta at the ninth line that shifts from historical context to contemporary relevance. Additionally, incorporate a classical allusion to a Greek poet, and use at least one example of anaphora.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import gradio as gr\nfrom transformers import pipeline\n\n# Load model and tokenizer using pipeline for more efficient memory management\npipe = pipeline(\n    \"text-generation\",\n    model=\"microsoft/phi-3-mini-4k-instruct\",\n    tokenizer=\"microsoft/phi-3-mini-4k-instruct\",\n    device_map=\"auto\",  # Let Transformers automatically choose the best device\n    torch_dtype=\"float16\",  # Use half-precision for faster inference\n    trust_remote_code=True\n)\n\n# Predefined conversation responses\nconversations = {\n    \"who built this\": \"This application was built by Adewuyi Ayomide, a passionate Machine Learning Engineer and Computer Science student at the University of Ibadan. He specializes in Natural Language Processing and has a keen interest in making AI more accessible and creative.\",\n    \"who built this application\": \"This application was built by Adewuyi Ayomide, a passionate Machine Learning Engineer and Computer Science student at the University of Ibadan. He specializes in Natural Language Processing and has a keen interest in making AI more accessible and creative.\",\n    \"who built this?\": \"This application was built by Adewuyi Ayomide, a passionate Machine Learning Engineer and Computer Science student at the University of Ibadan. He specializes in Natural Language Processing and has a keen interest in making AI more accessible and creative.\",\n    \"who built this application?\": \"This application was built by Adewuyi Ayomide, a passionate Machine Learning Engineer and Computer Science student at the University of Ibadan. He specializes in Natural Language Processing and has a keen interest in making AI more accessible and creative.\",\n    \"who created you\": \"I was created by Adewuyi Ayomide, a talented Machine Learning Engineer and Computer Science student at the University of Ibadan. He developed me to help people explore the beauty of poetry through AI.\",\n    \"who created you?\": \"I was created by Adewuyi Ayomide, a talented Machine Learning Engineer and Computer Science student at the University of Ibadan. He developed me to help people explore the beauty of poetry through AI.\",\n    \"hey\": \"Hello! ğŸ‘‹ I'm your AI poetry companion. Would you like me to create a poem for you?\",\n    \"hi\": \"Hi there! ğŸ‘‹ Ready to explore the world of poetry together?\",\n    \"hello\": \"Hello! ğŸ‘‹ I'm excited to create some poetry with you today!\",\n    \"help\": \"I can help you create beautiful poems! Just share a topic, emotion, or idea, and I'll craft a unique poem for you. You can also ask me about who created me or just chat casually.\",\n    \"wow\": \"Thank you! I'm glad you're impressed. Would you like me to create another poem for you? Just share any topic that interests you!\",\n    \"amazing\": \"I'm delighted you think so! Would you like to explore more poetry together? Just give me a theme or emotion to work with!\",\n    \"awesome\": \"Thank you for the kind words! I enjoy creating poems. What topic would you like me to write about next?\",\n    \"beautiful\": \"I'm happy you enjoyed it! Poetry is a beautiful way to express emotions. Would you like another poem?\",\n    \"nice\": \"Thank you! I'm here to create more poems whenever you're ready. Just share a topic with me!\",\n    \"great\": \"I'm glad you liked it! Ready for another poetic journey? Just give me a theme to work with!\",\n    \"good\": \"Thank you! I enjoy crafting poems. Would you like to try another topic?\",\n    \"thank you\": \"You're welcome! It's my pleasure to create poems. Feel free to request another one whenever you'd like!\",\n    \"thanks\": \"You're welcome! Ready for another poem whenever you are!\"\n}\n\ndef generate_poem(prompt, history=None):\n    if history is None:\n        history = []  # Initialize history as an empty list if None is passed\n\n    # Check for predefined conversation responses\n    prompt_lower = prompt.strip().lower()\n    if prompt_lower in conversations:\n        response = conversations[prompt_lower]\n        history.append((\"You\", prompt))\n        history.append((\"AI\", response))\n        return history, history\n\n    # Ensure the prompt is not empty\n    if not prompt.strip():\n        return history + [(\"You\", \"Please provide a topic or idea for the poem.\")], history\n\n    try:\n        # Generate the poem using the pipeline with optimized parameters\n        outputs = pipe(prompt, max_new_tokens=500, temperature=0.7, do_sample=True)\n        poem = outputs[0]['generated_text']  # Extract generated text from pipeline output\n    except Exception as e:\n        poem = f\"An error occurred: {e}\"\n\n    # Add the user prompt and the poem response to the history\n    history.append((\"You\", prompt))\n    history.append((\"AI\", poem))\n\n    return history, history\n\n# Create the Gradio interface\ninterface = gr.Interface(\n    fn=generate_poem,\n    inputs=[\"text\", \"state\"],\n    outputs=[\"chatbot\", \"state\"],\n    title=\"Love Poem Generator Chatbot\",\n    description=\"Chat with the AI, and it will generate love poems for you!\"\n)\n\n# Launch the Gradio interface\ninterface.launch(share=True)  # Use share=True to get a public link\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T13:21:41.328638Z","iopub.execute_input":"2024-12-20T13:21:41.329062Z","iopub.status.idle":"2024-12-20T13:21:51.115179Z","shell.execute_reply.started":"2024-12-20T13:21:41.329027Z","shell.execute_reply":"2024-12-20T13:21:51.114243Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d32d446df22941d3a7287d08ed5ac3e5"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/gradio/components/chatbot.py:222: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7863\n* Running on public URL: https://bf31fba69ed349d4ea.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://bf31fba69ed349d4ea.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}},{"name":"stderr","text":"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","output_type":"stream"}],"execution_count":6}]}